---
title: "Estudo de Python e dados II"
subtitle: "Manipulação de dados: Preparação de dados, dados ausentes e Tidy data"
author: "Sergio Pedro Rodrigues Oliveira"
date: last-modified
date-format: DD MMMM YYYY
lang: pt
format:
    html:
        code-fold: true
        number-sections: true
    pdf:
      toc: false
      lof: false
      lot: false
      toc-depth: 5
      number-sections: true
      number-depth: 5
      colorlinks: true
      cite-method: biblatex
    docx:
      toc: true
      number-sections: true
      highlight-style: github
jupyter: python3
bibliography: Quarto/pythonbibliografia.bib
csl: Quarto/abnt.csl
---

\thispagestyle{empty}

\newpage
\pagenumbering{roman}

```{=latex}
\setcounter{tocdepth}{4}
\renewcommand{\contentsname}{SUMÁRIO}
\tableofcontents
```

\newpage

```{=latex}
\setcounter{tocdepth}{4}
\renewcommand{\listfigurename}{LISTA DE FIGURAS}
\listoffigures
```
\newpage

```{=latex}
\setcounter{tocdepth}{4}
\renewcommand{\listtablename}{LISTA DE TABELAS}
\listoftables
```

```{python}
#| echo: false
#| error: false
#| warning: false

# Bibliotecas para o funcionamento do documento Quarto

from IPython.display import Markdown
from tabulate import tabulate
import math
import statistics
import numpy as np
import pandas as pd
import json
import matplotlib.pyplot as plt
import seaborn as sns
```

\newpage
\pagenumbering{arabic}

# Objetivo

O objetivo deste estudo é explorar e documentar as funcionalidades essenciais das principais bibliotecas científicas do Python, como `NumPy`, `Pandas` e outras, através de exemplos práticos e casos de uso selecionados. Pretende-se consolidar o conhecimento sobre a manipulação, análise e visualização de dados, servindo como um guia de referência pessoal para futuros projetos de programação científica.

\newpage

# Preparação dos dados

## Introdução

A essa altura, você deverá ser capaz de carregar dados no `Pandas` e fazer algumas visualização básica. Essa parte do livro tem como foco várias tarefas de limpeza dos dados. Começaremos com a preparação de um conjunto de dados para análise por meio da combinação de diversos conjuntos.

### Mapa conceitual {.unnumbered}

#. Conhecimento prévio
   #. Carga de dados;
   #. Obtenção de subconjuntos de dados;
   #. Funções e métodos de classe.

### Objetivos {.unnumbered}

Este capítulo abordará:
   
   #. *Tidy data* (dados organizados);
   
   #. Concatenação de dados;
   
   #. Combinação (merge) de conjunto de dados.

```{mermaid}
%%| label: fig-prepdados
%%| fig-cap: "Preparação de dados, principais tópicos."

graph TD
    A[Preparação dos dados] --> B[Tidy Data]
    A --> C[Concatenação de dados]
    A --> D[merge]
```

\newpage

## Tidy data

Hadley Wickham, um dos mais proeminentes membros da comunidade **R**, fala sobre a ideia de *tidy data* (dados organizados). 

Com efeito, ele escreveu um artigo sobre esse conceito no *Journal of Statistical Software*. *Tidy data* é um framework para estruturar [conjuntos de dados]{.underline} a fim de que sejam facilmente [analisados]{.underline}. É usado principalmente como um objetivo a que devemos visar quando [limpamos os dados]{.underline}. Depois que você compreender o que é o conceito de *tidy data*, esse conhecimento fará com que a [coleta de dados]{.underline} seja muito mais fácil.

Então o que é *tidy data*? O artigo de Hadley Wickham o define como um conceito que atende aos seguintes critérios:

#. "**Cada observação deve formar uma linha**" (Observation)

   Uma observação é o conjunto de todas as medidas feitas em uma única unidade (ex: uma pessoa em um exame, um país em um ano específico).

   * O erro comum: Repetir a mesma observação em várias colunas ou espalhar os dados de uma mesma pessoa em tabelas diferentes sem necessidade.

   * O modo Tidy: Se você está analisando a saúde de pacientes, cada linha deve representar um paciente em um momento específico.

#. "**Cada variável deve formar uma coluna**" (Variable)

   Uma variável é um atributo que você mede (ex: Peso, Data, Temperatura).

   * O erro comum: Ter colunas chamadas "Janeiro", "Fevereiro" e "Março". Aqui, o nome da variável é "Mês", e Janeiro/Fevereiro são apenas valores.

   * O modo Tidy: Criar uma coluna única chamada Mes onde os valores são listados.

#. "**Cada tipo de unidade observacional forma uma tabela**"

   Esta regra foca na [organização macro]{.underline}.

   Sobre o terceiro critério do tidy data, [as observações devem ser coerentes com a tabela]{.underline}, tornando-a objetiva quanto ao tipo de informação que deve armazenar. A ideia é que [a tabela tenha um propósito único]{.underline}; ao misturar assuntos diferentes em uma mesma estrutura, fere-se a normalização dos dados.

   * O erro comum: Misturar dados de "Clientes" com dados de "Vendas" na mesma tabela, causando redundância (ex: repetir o endereço do cliente toda vez que ele compra algo).

   * O modo Tidy: Ter uma tabela para Clientes e outra para Vendas, relacionando-as por um ID. Isso facilita a manutenção e evita erros de digitação.

\newpage

### Combinando conjuntos de dados

Começaremos com o último critério de Hadley Wickham para *tidy data*: "cada tipo de unidade de observação forma uma tabela".

Quando os dados estão organizados, é necessário combinar várias tabelas para responder a uma pergunta. Por exemplo, pode haver uma tabela separada que armazene informações de empresas e outra tabela contendo preços de ações. Se quisermos observar os preços de todas as ações no mercado de tecnologia, talvez antes tenhamos de encontrar todas as empresas de tecnologia na tabela de informações sobre empresas e então combinar esses dados com os preços das ações a fim de obter as informações de que precisamos para responder à nossa pergunta.

Os dados podem ter sido separados em tabelas distintas para reduzir a quantidade de informações redundantes (não precisamos armazenar informações sobre as empresas em cada entrada de preço das ações), mas essa organização implica que, como analistas de dados, teremos de combinar os dados relevantes por conta própria para responder à nossa pergunta.

Em outras ocasiões, um único conjunto de dados pode estar dividido em várias partes. Por exemplo, em dados de séries temporais, cada data pode estar em um arquivo separado. Em outro caso, um arquivo pode ter sido separado em partes para que os arquivos individuais fossem menores. Talvez você precise combinar dados de diversas origens para responder a uma pergunta (por exemplo, como combinar latitudes em longitudes com CEPs). Nos dois casos, você terá que combinar dados em um único dataframe de análise.

\newpage

## Concatenação

Uma das maneiras mais (conceitualmente) fáceis de combinar dados é por meio da concatenação.

Podemos pensar na concatenação como uma junção de linhas ou colunas em seus dados. Essa abordagem é possível se seus dados estiverem separados em partes ou se você fez um cálculo que queira concatenar ao seu conjunto de dados existente.

A [concatenação]{.underline} é feita usando a [função `concat` do `Pandas`]{.underline}.

### Adicionando linhas

Vamos começar com alguns conjuntos de dados de exemplo para que você veja o que realmente acontece.

```{python}
import pandas as pd

df1 = pd.read_csv("Cap_04-Preparacao_dados/01-Concatenacao/concat_1.csv")
df2 = pd.read_csv("Cap_04-Preparacao_dados/01-Concatenacao/concat_2.csv")
df3 = pd.read_csv("Cap_04-Preparacao_dados/01-Concatenacao/concat_3.csv")

print("dataframe_csv_1:")
print(df1)
print("dataframe_csv_2:")
print(df2)
print("dataframe_csv_3:")
print(df3)
```

A operação de empilhar dataframes uns sobre os outros é feita com a função `concat` do `Pandas`. Todos os dataframes a serem concatenados são passados em uma `list`.

```{python}
row_concat = pd.concat([df1,df2,df3])
print(row_concat)
```

Como podemos ver, `concat` empilha cegamente os dataframes. Se observar os nomes das linhas (isto é, seus índices), verá que eles são apenas uma versão empilhada dos índices originais das linhas.

Se aplicarmos os diversos métodos para obtenção de subconjuntos, os subconjuntos serão obtidos conforme esperado.

```{python}
#Obtém o subconjunto da quarta linha do dataframe concatenado
print(row_concat.iloc[3,])
```

\newpage

O que acontece se você usar `loc` para obter o subconjunto do novo dataframe?

A função `loc` pega os subconjuntos pelo rótulo (nome da linha), logo pega todos os rótulos com mesmo nome, pegando assim 3 linhas diferentes com mesmo nome.

```{python}
print(row_concat.loc[3])
```

\newpage

#### Concatenar `Series`

Anteriormente apresentamos o processo de criar uma `Series`. No entanto, se criássemos uma nova série para concatenar em um dataframe, ela não seria concatenada corretamente.

```{python}
#Criar uma nova linha de dados
new_row_series = pd.Series(['n1','n2','n3','n4'])
print(new_row_series)
```

```{python}
#Tentando adicionar a nova linha em um dataframe
print(pd.concat([df1,new_row_series]))
```

O primeiro detalhe que você perceba são os valores [`NaN`]{.underline}. É simplesmente o modo Python de representar um ["valor ausente"]{.underline}.

Esperávamos concatenar nossos novos valores como uma linha, mas isso não aconteceu. De fato, nosso código não só não concatenou os valores como uma linha, como também criou uma nova coluna totalmente desalinhada em relação ao restante dos dados.

Se pararmos para pensar no que está acontecendo nesse caso, poderemos ver que o resultado, na verdade, faz sentido. Em primeiro lugar, se observarmos os novos índices adicionados, percebemos que são muito semelhantes aos resultados que obtivemos quando concatenamos dataframes antes. Os índices do objeto `new_row_series` são análogos aos números das linhas do dataframe.

Além disso, como nossa série não tem uma coluna correspondente, nosso `new_row_series` foi adicionado em uma nova coluna.

\newpage

[Para corrigir esse problema]{.underline}, podemos [transformar a nossa série em um dataframe]{.underline}. Esse dataframe contém uma linha de dados, e os nomes das colunas são aqueles com as quais os dados serão associados.

```{python}
#Observe os colchetes duplos
new_row_df = pd.DataFrame([['n1','n2','n3','n4']],columns=['A','B','C','D'])
print(new_row_df)
```

```{python}
print(pd.concat([df1,new_row_df]))
```

\newpage

`concat` é uma função genérica capaz de concatenar vários [dados de uma só vez]{.underline}.

Se você tiver de [concatenar um único objeto a um dataframe existente]{.underline}, a função `append` poderá cauidar dessa tarefa.

[O método `.append()` foi descontinuado]{.underline} nas versões mais recentes do Pandas (acima da 2.0). Atualmente, a forma correta e recomendada de juntar DataFrames é usar o pd.concat().

* Usando um `DataFrame`:

```
print(df1.append(df2))
```

* Usando um `DataFrame` com uma só linha:

```
print(df1.append(new_row_df))
```

* Usando um dicionário Python:

```
data_dict = {
   'A': 'n1',
   'B': 'n2',
   'C': 'n3',
   'D': 'n4'
}

print(df1.append(data_dict, ignore_index=True))
```

\newpage

#### Ignorando o índice

No último exemplo, quando adicionamos um `dict` em um dataframe, tivemos que usar [o parâmetro `ignore_index`]{.underline}. Se observarmos com mais atenção, veremos que [o índice da linha também foi incrementado em 1]{.underline}, e não houve repetição de um valor de índice anterior.

Se simplesmente queremos concatenar os dados, podemos usar o parâmetro [`ignore_index` para reiniciar o índice da linha após a concatenação]{.underline}.

```{python}
row_concat_i = pd.concat([df1,df2,df3],
ignore_index=True)
print(row_concat_i)
```

\newpage

### Adicionando colunas

#### Concatenando colunas - axis {.unnumbered}

Concatenar colunas é muito semelhante a concatenar linhas. A principal diferença esta no parâmetro `axis` da função `concat`.

O valor [default de `axis` é `0`]{.underline}, portanto ele concatenará os [dados nas linhas]{.underline}. Entretanto, se passarmos [`axis=1`]{.underline} para a função, [os dados serão concatenados nas colunas]{.underline}.

```{python}
col_concat = pd.concat([df1,df2,df3],
axis=1)
print(col_concat)
```

#### Subconjuntos colunas {.unnumbered}

Se tentarmos obter um [subconjunto de dados com base nos nomes das colunas]{.underline}, teremos um resultado similar aquele obtido se concatenássemos por linha e gerássemos um subconjunto pelo índice da linhas.

```{python}
print(col_concat['A'])
```

\newpage

#### Adicionar uma única coluna {.unnumbered}

[Adicionar uma única coluna]{.underline} em um dataframe pode ser feito diretamente, [sem usar nenhuma função específica do Pandas]{.underline}.

Basta específicar [um novo nome de coluna e o vetor]{.underline} que você quer que seja atribuído a essa nova coluna.

```{python}
col_concat['new_col_list'] = ['n1','n2','n3','n4']
print(col_concat)
```

```{python}
col_concat['new_col_series'] = pd.Series(['n1','n2','n3','n4'])
print(col_concat)
```

Usar a função `concat` continua funcionando, desde que você lhe passe um dataframe. Essa abordagem exige um pouco mais de código desnecessário.

\newpage

#### Reiniciando índices colunas {.unnumbered}

Por fim, podemos [reiniciar os índices das colunas]{.underline} para que [não tenhamos nomes duplicados]{.underline}.

```{python}
print(pd.concat([df1,df2,df3], axis=1, ignore_index=True))
```

\newpage

### Concatenação com índices diferentes

Os exemplos apresentados [até agora partiram de pressuposto]{.underline} de que estávamos executando uma [concatenação simples de linha ou de coluna]{.underline}. Também foi suposto que a(s) nova(s) linha(s) tinha(m) os mesmos nomes de colunas ou que a(s) coluna(s) tinha(m) os mesmos índices de linha.

Esta seção aborda o que acontece quando os índices das linhas e das colunas não estão alinhados.

#### Concatenando linhas com colunas diferentes

Vamos modificar nossos dataframes para os próximos exemplos:

```{python}
df1.columns = ['A','B','C','D']
df2.columns = ['E','F','G','H']
df3.columns = ['A','C','F','H']
```

```{python}
print(df1)
```

```{python}
print(df2)
```

```{python}
print(df3)
```

\newpage

Se tentarmos concatenar esses dataframes como fizemos anteriormente, os dataframes agora serão muito mais do que simplesmente empilhados uns sobre os outros. [As colunas se alinharão e `NaN` preencherá qualquer área que estaja faltando]{.underline}.

```{python}
row_concat = pd.concat([df1,df2,df3])
print(row_concat)
```

Uma maneira de evitar a inclusão de valores `NaN` é manter [somente as colunas que sejam compartilhadas pela lista de objetos a serem concatenados]{.underline}. Um parâmetro chamado `join` faz isso.

Por padrão, seu valor é `outer`, o que significa que todas as colunas serão mantidas.

Porém, [podemos definir `join='inner'` para manter somente as colunas que sejam compartilhadas entre os conjuntos de dados]{underline}.

Se tentarmos manter apenas as colunas de todos os três dataframes, teremos um dataframe vazio, pois não há nenhuma coluna em comum.

```{python}
print(pd.concat([df1,df2,df3], join='inner'))
```

\newpage

[Se usarmos os dataframes que tenham colunas em comum, somente aquelas que sejam compartilhadas por todos serão devolvidas]{.underline}.

```{python}
print(pd.concat([df1,df3],ignore_index=False, join='inner'))
```

\newpage

#### Concatenando colunas com linhas diferentes

Vamos pegar nossos dataframes e modificá-los novamente de modo que tenham índices de linha diferente. Nesse caso, estamos tomando como base as mesmas modificações de dataframe feitas anteriormente.

```{python}
df1.index = [0,1,2,3]
df2.index = [4,5,6,7]
df3.index = [0,2,5,7]
```

```{python}
print(df1)
```

```{python}
print(df2)
```

```{python}
print(df3)
```

\newpage

Quando concatenamos ao longo de `axis=1`, temos o mesmo resultado de concatenar ao longo de `axis=0`. Os novos dataframes serão somados por coluna, havendo correspondência entre seus respectivos índices de linha. Indicadores de valores ausentes aparecerão nas áreas em que os índices não se alinham.

```{python}
col_concat = pd.concat([df1,df2,df3], axis=1)
print(col_concat)
```

Assim como fizemos quando concatenamos por linha, podemos optar por [manter o resultado somente quando houver índices correspondentes]{.underline} usando [`join=inner`]{.underline}.

```{python}
print(pd.concat([df1,df3],axis=1,join='inner'))
```

\newpage

## Combinando vários conjuntos de dados

A seção anterior fez alusão a alguns conceitos de banco de dados. Os parâmetros `join='inner'` e o default `join='outer'` têm origem no modo de trabalhar com banco de dados quando queremos combinar tabelas.

Em vez de simplesmente ter um índice de linha ou de coluna que queremos usar para concaternar valores, às vezes podemos ter dois ou mais dataframes que queremos combinar com base em valores de dados comuns. Essa tarefa é conhecida no mundo dos bancos de dados como a execução de uma "junção" (`join`).

O Pandas tem um comando `pd.join` que usa `pd.merge` internamente. `join` fará a combinação (`merge`) de objetos com base em um índice, mas o comando `merge` é muito mais explícito e flexível. Se você planeja combinar dataframes pelo índice da linha, por exemplo, poderá consultar a função `join`.

Usaremos conjuntos de dados de pesquisa nessa série de exemplos.

```{python}
person = pd.read_csv('./Data/Cap_04/survey_person.csv')
site = pd.read_csv('./Data/Cap_04/survey_site.csv')
survey = pd.read_csv('./Data/Cap_04/survey_survey.csv')
visited = pd.read_csv('./Data/Cap_04/survey_visited.csv')
```

```{python}
print(person)
```

```{python}
print(site)
```

\newpage

```{python}
print(survey)
```

```{python}
print(visited)
```

\newpage

No momento, nossos dados estão separados em várias partes, cada uma sendo uma unidade de observação. Se quiséssemos observar as datas de cada local, junto com as informações de latitude e longitude desse local, teríamos de [combinar (e fazer um `merge`) de vários dataframes]{.underline}.Isso pode ser feito com a [função `merge` do Pandas]{.underline}. `merge` é, na verdade, um método de `dataframe`.

Quando chamamos esse método, o dataframe chamado será referenciado como '`left`'. Na função `merge`, o primeiro parâmetro é o dataframe '`right`'. O próximo parâmetro indica como o resultado final combinado se parecerá (`how`). A @tbl-pandassql apresenta mais detalhes.

Em seguida, definimos o parâmetro `on`. Ele especifica com quais colunas será feita a correspondência. Se as colunas a esquerda e à direita não tiverem o mesmo nome, poderemos usar os parâmetros `left_on` e `right_on` em seu lugar.

```{python}
#| echo: false
#| error: false
#| warning: false
#| label: tbl-pandassql
#| tbl-cap: Como o parâmetro *how* do **Pandas** se relaciona com o **SQL**

from IPython.display import Markdown
from tabulate import tabulate
table = [["`left`","`left outer`","Mantém todas as **chaves da esquerda**."],
         ["`right`","`right outer`","Mantém todas as **chaves da direita**."],
         ["`outer`","`full outer`","Mantém todas as **chaves** tanto da **esquerda** quanto da **direita**."],
         ["`inner`","`inner`","Mantém **somente** as **chaves** que existem \n**tanto na esquerda quanto na direita**."]
         ]
Markdown(tabulate(
  table, 
  headers=["**Pandas**","**SQL**","Descrição"],
  colalign=("left","left","left")
  ))
```

```{python}
#| echo: false
#| error: false
#| warning: false
#| label: tbl-how-examples
#| tbl-cap: Comportamento do parâmetro `how` no merge

from IPython.display import Markdown
from tabulate import tabulate

table = [
    ["`inner` (padrão)", "Interseção", "Retorna apenas as linhas que possuem chaves \nem ambos os DataFrames."],
    ["`left`","Prioriza a Esquerda", "Mantém todos os dados do DataFrame \nda esquerda e traz o que houver match da direita."],
    ["`right`","Prioriza a Direita", "Mantém todos os dados do DataFrame \nda direita e traz o que houver match da esquerda."],
    ["`outer`","União", "Retorna todos os registros de ambos. \nOnde não houver correspondência, preenche com `NaN`."]
]

Markdown(tabulate(
  table, 
  headers=["Tipo de Join", "Conceito", "Resultado Prático"],
  colalign=("left", "left", "left")
))
```

\newpage

```{python}
#| echo: false
#| error: false
#| warning: false
#| label: tbl-mergeparam
#| tbl-cap: Parâmetros do `merge`

from IPython.display import Markdown
from tabulate import tabulate
table = [["`left`","O primeiro DataFrame (base principal no caso de `left join`)."],
         ["`right`",'O segundo DataFrame (o que será "anexado").'],
         ["`how`","O tipo de união: '`left`', '`right`', '`inner`' (padrão) ou '`outer`'."],
         ["`on`","Nome da coluna usada como chave (quando o nome é igual em ambos)."],
         ["`left_on`","Nome da coluna chave no DataFrame da **esquerda**."],
         ["`right_on`","Nome da coluna chave no DataFrame da **direita**."],
         ["`suffixes`","Sufixos para diferenciar colunas com nomes iguais que não são chaves \n(ex: `_x`, `_y`)."]
         ]
Markdown(tabulate(
  table, 
  headers=["Parâmetro","Função"],
  colalign=("left","left")
  ))
```

\newpage

### `merge` um a um (one-to-one)

No tipo mais simples de `merge`, temos dois dataframes em que queremos fazer a [junção de uma coluna com outra]{.underline}, e as colunas que queremos juntar não contém [nenhum valor duplicado]{.underline}.

Nesse exemplo, modificamos o dataframe `visited` de modo que não haja valores duplicados de `site`.

```{python}
visited_subset = visited.loc[[0,2,6]]
print(visited_subset)
```

Podemos fazer nosso `merge` um a um (***one-to-one***):

```{python}
#O valor default de 'how' é 'inner',
# portanto, não precisa ser especificado.
o2o_merge = site.merge(
   visited_subset,
   left_on='name',
   right_on='site'
)

print(o2o_merge)
```

Como pode ver, criamos agora um novo dataframe a partir de dois dataframes diferentes, em que houve uma correspondência de linhas com base em um conjunto particular de colunas. [No jargão de **SQL**, as colunas usadas na correspondência são chamadas de "**chaves**"]{.underline}.

\newpage

### `merge` de muitos para um (many-to-one)

Se optarmos por fazer o mesmo `merge`, mas dessa vez sem usar o dataframe `visited` como subconjunto, fariamos um [`merge` de muitos para um (***many-to-one***)]{.underline}. Nesse tipo de merge, um dos dataframes tem valores de chave que se repetem.

[Os dataframes contendo as observações únicas serão então duplicados no `merge`]{.underline}.

```{python}
m2o_merge = site.merge(
   visited,
   left_on='name',
   right_on='site'
)

print(m2o_merge)
```

Como podemos ver, as informações de `site`(`name`, `lat` e `long`) foram duplicadas e houve correspondência com os dados de `visited`.

\newpage

### `merge` de muitos para muitos (many-to-many)

Por fim, há ocasiões em que vamos querer efetuar uma correspondência baseada em várias colunas. Como exemplo, suponha que haja dois dataframes, provenientes de um `merge` entre `person` e `survey` e de outro entre `visited` com `survey`.

```{python}
ps = person.merge(
   survey,
   left_on='ident',
   right_on='person'
)

vs = visited.merge(
   survey,
   left_on='ident',
   right_on='taken'
)
```

```{python}
print(ps)
```

```{python}
print(vs)
```

\newpage

Podemos executar um merge de muitos para muitos (***many-to-many***) passando as várias colunas com as quais a correspondência será feita, usando uma lista Python.

```{python}
ps_vs = ps.merge(
   vs,
   left_on=['ident','taken','quant','reading'],
   right_on=['person','ident','quant','reading']
)
```

Vamos observar apenas a primeira linha:

```{python}
print(ps_vs.loc[0,])
```

[O Pandas acrescentará automaticamente um sufixo no nome de uma coluna se houver colisões no nome]{.underline}. Na saída, `_x` refere-se aos valores do dataframe à esquerda, enquanto o sufixo `_y` é proveniente dos valores do dataframe à direita. 

\newpage

### Resumo

O `pd.merge()` funciona de forma similar aos `JOIN`s do **SQL**. O que define o tipo de relação é a frequência com que as chaves de ligação aparecem em cada DataFrame.

1. ***One-to-One*** (Um-para-Um - o2o)

    Ocorre quando a chave de ligação é única em ambos os DataFrames. É como unir duas tabelas de informações diferentes sobre os mesmos indivíduos.

    * **A lógica**: Cada linha da Tabela A encontra exatamente uma linha correspondente na Tabela B.

    * **Impacto nas linhas**: O número de linhas do DataFrame resultante será igual ao número de chaves que coincidem. Nenhuma linha é duplicada; as colunas são apenas "esticadas" para o lado.

    Exemplo: Uma tabela com `ID` e `Nome`, e outra com `ID` e `Cargo`.

2. ***Many-to-One*** (Muitos-para-Um - m2o)

    Ocorre quando uma das tabelas possui chaves repetidas, mas a outra possui apenas chaves únicas. O pandas preserva as repetições e propaga os dados da tabela "única".

    * **A lógica**: Uma linha da Tabela B (o lado "One") corresponde a várias linhas da Tabela A (o lado "Many").

    * **Impacto nas linhas**: As linhas da Tabela A são preservadas. O conteúdo da linha correspondente na Tabela B é copiado para cada uma dessas linhas.

    * **Exemplo visual**: Se o "Vendedor 1" aparece em 10 linhas de vendas, os dados dele (nome, região) serão repetidos nessas 10 linhas no resultado final.

    Exemplo: Uma tabela de **Vendas** (onde o `ID_Vendedor` se repete) e uma tabela de **Vendedores** (onde cada `ID_Vendedor` é único).

3. ***Many-to-Many*** (Muitos-para-Muitos - m2m)

    Ocorre quando a chave de ligação se repete em ambos os DataFrames. O resultado é um produto cartesiano das linhas correspondentes.

    * **A lógica**: Uma chave aparece N vezes na Tabela A e M vezes na Tabela B.

    * **Impacto nas linhas**: O Pandas cria todas as combinações possíveis entre elas. O número de linhas resultantes para aquela chave específica será $N \times M$.

    * **Atenção**: Este é o caso mais perigoso, pois pode gerar um aumento explosivo no tamanho do seu DataFrame se você não estiver esperando por essas combinações.

    Exemplo: Uma tabela de Produtos em promoção e uma tabela de Lojas que vendem esses produtos. Se um produto aparece 3 vezes na Tabela A e 2 vezes na Tabela B, o resultado terá $3 \times 2 = 6$ linhas para essa chave.

\newpage

## Conclusão preparação dos dados

As vezes, [será preciso combinar diversas partes ou dados ou vários conjuntos de dados, conforme a pergunta que tiver tentano responder]{.underline}. Tenha em mente, porém, que os dados de que você precisar nas análises não estarão necessariamente no melhor formato usado para armazená-los.

Os dados de pesquisa usados no último exemplo estavam separados em quatro partes que precisam ser combinadas. Depois que fizemos o `merge` das tabelas, muitas informações redundantes apareceram nas linhas. [Do ponto de vista da armazenagem de dados e de sua entrada, cada uma dessas duplicações pode lever a erros e inconsistência nos dados]{.underline}. É isso que Hadley quis dizer quando afirmou que, nos dados organizados (***Tidy data***), "cada tipo de unidade de observação forma uma tabela".

\newpage

# Dados Ausentes

## Introdução

Raramente você receberá um conjunto de dados sem valor ausente. Há muitas representações para dados ausentes. Nos bancos de dados, são valores `NULL`; certas linguagens de programação usam `NA`, e, dependendo da origem de seus dados, valores ausentes podem ser uma string vazia, '', ou até mesmo valores númericos como `88` ou `99`. [O Pandas exibe valores ausentes como `NaN`]{.underline}.

### Mapa conceitual {.unnumbered}

1. Conhecimento prévio:
    a. Importação de bibliotecas;
    b. Fatiamento e indexação de dados;
    c. Uso de funções e métodos;
    d. Uso de parâmetros de funções.

### Objetivos {.unnumbered}

Este capítulo abordará:

   1. O que é um valor ausente
   2. Como os valores ausentes são criados
   3. Como recodificar e fazer cálculos com valores ausentes

\newpage

## O que é um valor `NaN`?

[O valor `NaN` no `Pandas` é proveniente do `numpy`]{.underline}. Valores ausentes podem ser usados ou exibidos de algumas maneiras em [Python - `NaN`, `NAN` ou `nan` -, mas são todos equivalentes]{.underline}.

```
# Basta importar os valores ausentes do numpy

from numpy import NaN, NAN, nan
```

[Valores ausentes são diferentes de outros tipos de dados, pois não são realmente iguais a nada]{.underline}. O dado está ausente, portanto não há um conceito de igualdade. `NaN` não é equivalente a 0 nem a uma string vazia, ''. 

Podemos demonstrar essa ideia em Python fazendo testes de igualdade:

```{python}
print(np.nan == True)
```

```{python}
print(np.nan == False)
```

```{python}
print(np.nan == 0)
```

```{python}
print(np.nan == '')
```

Valores ausentes também não são iguais a outros valores ausentes:

```{python}
print(np.nan == np.nan)
```

\newpage

O Pandas tem [métodos embutidos para testar um valor ausente]{.underline}:

```{python}
print(pd.isnull(np.nan))
```

O Pandas também tem [métodos para testar valores que não são ausentes]{.underline}:

```{python}
print(pd.notnull(np.nan))
```

```{python}
print(pd.notnull(42))
```

```{python}
print(pd.notnull('missing'))
```

\newpage

## De onde vêm os valores ausentes?

Podemos obter valores ausentes quando [carregamos um conjunto de dados]{.underline} com esses valores, ou do [processo de manipulação dos dados (***data munging***)]{.underline}.

### Carga de dados

Os dados de pesquisa que usamos anteriormente incluíam um conjunto de dados, `visited`, que continham dados ausentes. Quando os valores foram carregados, o Pandas encontrou automaticamente as células com dados ausentes e nos deu um dataframe com o valor `NaN` nas células apropriadas.

Na função `read_csv`, [três parâmetros estão relacionados com a leitura de valores ausentes]{.underline}:
   
* `na_values`; 
* `keep_default_na`;
* `na_filter`.

\newpage

#### `na_values` {.unnumbered}

O parâmetro `na_values` nos permite especificar valores ausentes ou `NaN` adicionais. Você pode passar uma str Python ou um objeto do tipo lista para que seja automaticamente codificados como valores ausentes quando o arquivo é lido.

É claro que valores ausentes default, como `NA`, `NaN`, ou `nan`, já estão disponíveis, e é por isso que esse parâmetro nem sempre é usado.

Alguns dados saudáveis podem codificar `99` como valor ausente; para especificar o uso desse valor, defina `na_values=[99]`.

[`na_values`: Permite que você adicione seus próprios marcadores (ex: '999', 'FALTA', 'INTERROGAÇÃO')]{.underline}.

```{python}
# Define o local em que estão os dados
visited_file = 'Data/Cap_04/survey_visited.csv'

# Carragar os dados com valores default
print(pd.read_csv(visited_file))
```

\newpage

#### `keep_default_na` {.unnumbered}

[O parâmetro `keep_default_na` é um `bool`]{.underline} que permite especificar se algum valor adicional deve ser considerado como ausente.

Esse valor é `True` por padrão, ou seja, qualquer valor ausente adicional especificado com o parâmetro `na_values` será concatenado à lista de valores ausentes. No entando, `keep_default_na` também pode ser definido com `keep_default_na=False`; nesse caso, somente os valores ausentes especificados em `na_values` serão usados.

[`keep_default_na=False` decide se a lista padrão do Pandas (ex: '`NaN`', '`NULL`', '`N/A`') deve ser usada]{.underline}.

```{python}
# Carrega os dados sem valores ausentes default
print(pd.read_csv(visited_file,keep_default_na=False))
```

```{python}
# Especifica manualmente os valores ausentes
print(pd.read_csv(
   visited_file,
   na_values=[''],
   keep_default_na=False))
```

\newpage

#### `na_filter` {.unnumbered}

Por fim, `na_filter` é um `bool` que especificará se algum valor será lido como ausente.

O valor default de `na_filter=True` implica que valores ausentes serão codificados como `NaN`. Se atribuirmos `na_filter=False`, então nada será recodificado como ausente. [Liga e desliga a busca por dados faltantes, se for `False`, os outros dois parâmetros são ignorados.]{.underline}.

Podemos pensar nesse parâmetro como uma forma de desativar todos os parâmetros definidos para `na_values` e `keep_default_na`, mas é mais provável que ele seja usado se quisermos ter um ganho de desempenho carregando dados sem valores ausentes.

Como ele funciona:

   * `True` (Padrão): O pandas analisa o conteúdo de todas as colunas em busca de strings que representem dados faltantes e as converte para `NaN`.

  * `False`: O pandas ignora essa busca. Tudo o que estiver no arquivo será lido literalmente como string ou número, sem conversão para "ausente".

Por que usar `na_filter=False`?

   * Velocidade: Em arquivos gigantes onde você sabe que não há dados faltantes, desativar o filtro acelera consideravelmente o carregamento.

   * Integridade: Se o seu arquivo contém a string "NA" (por exemplo, o código de um país como a Namíbia) e você não quer que o pandas a transforme em um buraco vazio (`NaN`).

\newpage

### Dados combinados

No capítulo Preparação dos dados mostrou como combinar conjunto de dados (`merge`).Alguns dos exemplos naquele capítulo inclíam valores ausentes na saída. Se recriarmos a tabela combinada, veremos valores ausentes na saída resultante.

```{python}
visited = pd.read_csv('Data/Cap_04/survey_visited.csv')
survey = pd.read_csv('Data/Cap_04/survey_survey.csv')
```

```{python}
print(visited)
```

```{python}
print(survey)
```

```{python}
# Default how = 'inner'
vs = visited.merge(
   survey, 
   left_on='ident',
   right_on='taken'
   )
print(vs)
```

\newpage

### Valores de entrada do usuário

O usuário tambem pode criar valores ausentes - por exemplo, criar um vetor de valores a partir de um cálculo ou um vetor preparado manualmente.

Criaremos nossos próprios dados com valores ausentes. `NaN`s são valores válidos tanto quanto para `Series` quanto para `DataFrames`.

```{python}
# Valores ausentes em uma Series
# Também poderia usar 'None' ao inves de pd.nan
num_legs = pd.Series({'goat':4, 'amoeba': None})

print(num_legs)
```

```{python}
# Valores ausentes em um dataframe
scientists = pd.DataFrame({
   'Name':['Rosaline Franklin','William Gosset'],
   'Occupation':['Chemist','statistician'],
   'Born':['1920-07-25','1876-06-13'],
   'Died':['1958-04-16','1937-10-16'],
   'missing': [None, np.nan]
})

print(scientists)
```

\newpage

Você também pode atribuir diretamente uma coluna de valores ausentes a um dataframe.

```{python}
# Cria um novo dataframe
scientists = pd.DataFrame({
   'Name':['Rosaline Franklin','William Gosset'],
   'Occupation':['Chemist','statistician'],
   'Born':['1920-07-25','1876-06-13'],
   'Died':['1958-04-16','1937-10-16']
})

# Atrabui uma coluna de valores ausentes
scientists['missing']=np.nan

print(scientists)
```

\newpage

### Reindexação

Outra maneira de introduzir valores ausentes em seus dados é reindexando o seu dataframe. Isso é conveniente quando queremos adicionar novos indices em nosso dataframe, mas queremos preservar seus valores originais. Um uso comum é quando o índice representa algum intervalo de tempo e queremos adicionar mais datas.

Em termos simples:

   * O que acontece: Você define uma nova lista de índices (ex: novas datas).

   * A consequência: O Pandas mantém os dados que já existiam e, para os novos índices que não possuem valor correspondente, ele insere automaticamente um NaN (valor ausente).

Se quiséssemos observar somente os anos 2000 a 2010 da plotagem de dados Gapminder, poderíamos executar asmesmas operações agrupadas, obter um subconjunto dos dados e então os reindexar.

```{python}
# Carregando arquivo CSV
gapminder = pd.read_csv('Data/Cap_01/gapminder.tsv', sep='\t')

# Agrupamento de dados
life_exp = gapminder.groupby(['year'])['lifeExp'].mean()

print(life_exp)
```

\newpage

Podemos reindexar fatiando os dados.

```
# Você pode continuar a encadear o 'loc' do código anterior
# Esse código não funciona mais!
print(life_exp.loc[range(2000,2010),])
```

No Pandas moderno (versão 1.0 ou superior), se você tentar usar o `.loc` com uma lista de rótulos onde alguns não existem no índice, o código gerará um erro de KeyError.

Antigamente, o Pandas criava linhas com NaN automaticamente, mas isso mudou para evitar erros silenciosos que poderiam causar confusão.

```{python}
# Em vez de .loc, use .reindex
indices_desejados = range(2000, 2010)
print(life_exp.\
   reindex(indices_desejados))
```

\newpage

Como alternativa, podemos obter o subconjunto dos dados separadamente e usar o método `reindex`.

```{python}
# Obtém subconjunto
y2000 = life_exp[life_exp.index > 2000]
print(y2000)
```

```{python}
# Reindexa
print(y2000.reindex(range(2000,2010)))
```

\newpage

## Trabalhando com dados ausentes

Agora que sabemos como os dados ausentes podem ser criados, vamos ver como eles se comportam quando estamos trabalhando com dados.

### Encontrando e contando dados ausentes

```{python}
ebola = pd.read_csv('Data/Cap_04/country_timeseries.csv')
```

#### Linhas {.unnumbered}

Uma maneira de ver o número de valores ausentes é contá-los com `count`.

```{python}
print(ebola.count())
```

\newpage

Você também pode subtrair o número de linhas não ausentes do número total de linhas.

```{python}
num_rows = ebola.shape[0]
num_missing = num_rows - ebola.count()
print("Número de linhas ausentes:")
print(num_missing)
```

\newpage

#### Colunas {.unnumbered}

Se quiser contar o número total de valores ausentes em seus dados, ou contar o número de valores ausentes em uma coluna em particular, você poderá usar a função `count_nonzero` do `numpy` em conjunto com o método `isnull`.

A função `np.count_nonzero()` conta quantos valores são verdadeiros (`True`). Como o método `.isnull()` retorna `True` para cada dado ausente, a combinação dos dois resulta no total de nulos.

```{python}
print(np.count_nonzero(ebola.isnull()))
```

```{python}
print(np.count_nonzero(ebola['Cases_Guinea'].isnull()))
```

\newpage

Outra forma de obter os contadores de dados ausentes é usar o método `value_counts` em uma série. Uma tabela de frequência de valores será exibida. Se o parâmetro `dropna` for usado, você também poderá obter um contador de valores ausentes.

O `value_counts` serve para contar quantas vezes cada valor aparece em uma coluna. Imagine uma coluna com as frutas: [Maçã, Maçã, Banana, NaN].
O que acontece na prática:

   * Sem o `dropna=False` (Comportamento padrão): O Pandas ignora os dados ausentes. Ele diria que você tem 2 Maçãs e 1 Banana. O `NaN` fica invisível.

   * Com o `dropna=False`: O Pandas passa a tratar o dado ausente como uma categoria comum. O resultado seria:
      
     * Maçã: 2

     * Banana: 1

     * NaN: 1

```{python}
# Obtém os 5 primeiros contadores da coluna Cases_Guinea
print(ebola.Cases_Guinea.value_counts(dropna=False).head())
```

```{python}
# Também poderia ser
print(ebola['Cases_Guinea'].value_counts(dropna=False).head())
```

\newpage

### Limpando dados ausentes

Há muitas maneiras com as quais podemos lidar com dados ausentes. Por exemplo, podemos [substituir os dados ausentes por outro valor]{.underline}, [preenchê-los usando dados existentes]{.underline} ou [descartá-los de nosso conjunto de dados]{.underline}.

```{python}
#| echo: false
#| error: false
#| warning: false
#| label: tbl-limpezadados
#| tbl-cap: Métodos para limpeza de dados.

from IPython.display import Markdown
from tabulate import tabulate
table = [['`fillna`',"Recodificar/substituir valores ausentes.","`df.fillna(0)`"],
         ["`method='ffill'`","Método embutido para fazer \npreenchimento para a frente (forward).","`.fillna(method='ffill')`"],
         ["`method='bfill'`","Método embutido para fazer \npreenchimento para trás (backward).","`.fillna(method='bfill')`"],
         ["`interpolate`","A interpolação usa valores existentes \npara preencher dados ausentes.","`.interpolate()`"],
         ["`dropna`","Descartar observações ou variáveis \ncom esses dados ausentes.",".dropna()"]]
Markdown(tabulate(
  table, 
  headers=["Método","Descrição","Exemplo"],
  colalign=("left","left","left")
  ))
```

\newpage

#### Recodificar/substituir

Podemos usar [o método `fillna`]{.underline} para recodificar os valores ausentes, [transformando-os em outro valor]{.underline}.

Por exemplo, suponha que quiséssemos que os valores ausentes fossem recodificados como 0.

```{python}
print(ebola.fillna(0).iloc[0:10,0:5])
```

Ao usar `fillna`, podemos recodificar os valores para um valor específico. 

Se consultar a documentação, você descobrirá que `fillna`, como muitas outras funções Pandas, tem um parâmetro `inplace`. Isso significa que os dados subjacentes serão automaticamente alterados para você; não é necessário criar uma nova cópia com as alterações. Você poderá usar esse parâmetro quando seus dados ficarem maiores e quiser que o código seja mais eficiente no que diz respeito à memória.

Em termos simples:

   * Comportamento padrão: Quando você usa `df.fillna(0)`, o Pandas cria uma cópia inteira da sua tabela na memória com os novos valores. O original permanece intocado a menos que você o sobrescreva (`df = df.fillna(0)`).

   * Com `inplace=True`: O Pandas altera diretamente a tabela original. Ele não cria uma cópia nova.

Sintaxe do `inplace`:

```
df.fillna(0, inplace=True)
```

\newpage

#### Preenchimento para a frente (*forward*)

Podemos usar métodos embutidos para fazer preenchimentos para a frente (*forward*) ou para trás (*backward*). Quando preenchemos os dados para a frente, [o último valor conhecido é usado para o próximo valor ausente]{.underline}. Desse modo, os valores ausentes são substituídos pelo último valor conhecido/registrado.

```{python}
#| error: false
#| warning: false
print(ebola.fillna(method='ffill').\
   iloc[0:10,0:5])
```

Se a coluna começar com um valor ausente, esse dado permanecerá ausente, pois não haverá um valor anterior com o qual fazer o preenchimento.

**Lógica**: O valor da linha $n$ é copiado para a linha $n+1$ se esta for `NaN`.

\newpage

#### Preenchimento para trás (*backward*)

Também podemos fazer com que o Pandas preencha dados para trás. Quando preenchemos dados para trás, o valor mais recente é usado para substituir os dados ausentes. Desse modo, os valores ausentes são substituídos pelo valor mais recente.

```{python}
#| error: false
#| warning: false
print(ebola.fillna(method='bfill')\
   .iloc[:,0:5]\
      .tail())
```

Se uma coluna terminar com um valor ausente, esse dado permanecerá ausente, pois não haverá nenhum valor novo com o qual fazer o preenchimento.

**Lógica**: O valor da linha $n$ é preenchido pelo valor da linha $n+1$.

\newpage

#### Interpolação

A interpolação usa valores existentes para preencher valores ausentes. Embora haja muitas meneiras de preencher valores ausentes, a interpolação no Pandas preenche os valores ausentes de [modo linear]{.underline}. Especificamente, os valores ausentes são tratados como se devessem estar igualmente espaçados.

```{python}
#| error: false
#| warning: false
print(ebola.interpolate()\
   .iloc[0:10,0:5])
```

O método `interpolate` tem um parâmetro `method` que pode alterar o método de interpolação.

Principais Parâmetros:

   * `method`: Define a técnica matemática.

      * '`linear`': (Padrão) Ignora o índice e trata os valores como igualmente espaçados.

      * '`time`': Funciona com séries temporais, considerando o intervalo real de tempo entre as datas.

      * '`polynomial`' ou '`spline`': Requer que você passe uma order (ex: ordem 2 ou 3) para curvas mais suaves.

   * `limit_direction`: Controla a direção do preenchimento.

      * '`forward`'; 
      * '`backward`'; 
      * '`both`'.

   * `inplace`: Se `True`, altera o DataFrame original.

\newpage

#### Descartando valores ausentes

O último modo de trabalhar com dados ausentes é descartar observações ou variáveis com esses dados.

Podemos usar o método `dropna` para descartar dados ausentes e especificar parâmetros para que esse método que controlem como os dados são descartados.

Por exemplo, o parâmetro `how` permite especificar se uma linha (ou coluna) será descartada quando algum dado ('`any`') ou todos ('`all`') estiverem ausentes.

O parâmetro `thresh` permite especificar quantos valores diferentes de `NaN` devemos ter para descartar a linha ou a coluna.

```{python}
print(ebola.shape)
```

Se somente os casos de dados completos forem mantidos em nosso conjunto de dados Ebola, ficaremos com apenas uma linha de dados.

```{python}
ebola_dropna = ebola.dropna()
print(ebola_dropna.shape)
```

```{python}
print(ebola_dropna)
```

\newpage

**Principais parâmetros**:

* `axis`: 
  
  Define se você remove a linha (`0` ou '`index`') ou a coluna (`1` ou '`columns`'). O padrão é remover a linha.

* `how`:

    * '`any`': Remove se houver pelo menos um valor nulo (**padrão**).

    * '`all`': Só remove se todos os valores da linha/coluna forem nulos.

* `subset`: 
  
  Permite focar em colunas específicas.
  
  Ex: `df.dropna(subset=['idade'])` só removerá a linha se o nulo estiver na coluna "`idade`".

* `inplace=True`:
  
  Altera o DataFrame original diretamente, sem precisar atribuir a uma nova variável.

**Cuidado**:

Dependendo da quantidade de dados que estiver faltando, manter somente os casos com dados completos pode deixar você com um conjunto de dados inútil. Talvez os dados ausentes não sejam aleatórios, de modo que descartar valores ausentes fará com que você fique com um conjunto de dados tendenciosos ou, quem sabe, manter somente os dados completos o deixará com dados insuficientes para efetuar a sua análise.

\newpage

### Cálculos com dados ausentes

Supanha que quiséssemos ver os contadores de casos para várias regiões. Podemos somar vários regiões a fim de obter uma nova coluna que armazene os contadores de casos.

```{python}
ebola['Cases_multiple'] = ebola['Cases_Guinea']+\
   ebola['Cases_Liberia']+\
   ebola['Cases_SierraLeone']
```

Vamos observer as dez primeiras linhas de cálculo.

```{python}
ebola_subset = ebola.loc[:,\
   ['Cases_Guinea','Cases_Liberia',\
      'Cases_SierraLeone','Cases_multiple']]
print(ebola_subset.head(n=10))
```

Podemos ver que um valor para `Cases_multiple` foi calculado somente quando não havia valores ausentes para `Cases_Guinea`, `Cases_Liberia` e `Cases_SierraLeone`. [Cálculos com valor ausente em geral devolverão um valor ausente]{.underline}, a menos que a função ou método chamado tenha um modo de ignorar os valores ausentes em seus cálculos.

\newpage

Exemplos de métodos embutidos capazes de ignorar valores ausentes incluem `mean` e `sum`. Essas funções geralmente têm um parâmetro `skipna` permitindo que um valor ainda seja calculado, ignorando os valores ausentes.

```{python}
# Ignorar valores ausentes é True por padrão
print(ebola.Cases_Guinea.sum(skipna=True))
```

```{python}
print(ebola.Cases_Guinea.sum(skipna=False))
```

\newpage

## Conclusão dados ausentes

É raro ter um conjunto de dados sem qualquer valor ausente. É importante saber como trabalhar com valores ausentes porque, mesmo quando estiver trabalhando com dados completos, valores ausentes ainda poderão surgir durante a sua própria manipulação de dados.

Neste capítulo, analisamos alguns dos métodos básicos usados no processo de análise de dados pertinentes à validade dos dados. Ao observar seus dados e tabular os valores ausentes, podemos dar início ao processo de avaliar se os dados têm uma qualidade suficientemente boa para tomada de decisões e inferências.

\newpage

# *Tidy data* (dados organizados)

## Introdução

Conforme mencionamos anteriormente, Hadley Wickham, um dos membros mais proeminentes da comunidade R, introduziu o conceito de *tidy data* (dados organizados) em um artigo do *Journal of Statistical Software*.

[*Tidy data* é um framework para estruturar conjuntos de dados de modo que sejam facilmente analisados e visualizados]{.underline}. Podemos pensar nele como um objetivo a que devemos visar quando limpamos os dados.

Depois que você compreende o que é *tidy data*, esse conhecimento fará com que a análise, a visualização e a coleta de dados sejam muito mais fáceis.

Então o que é *tidy data*? O artigo de Hadley Wickham o define como um conceito que atende aos seguintes critérios:

* Cada linha é uma observação. (***observation***)
* Cada coluna é uma variável. (***variable***)
* Cada tipo de unidade de observação forma uma tabela.

Este capítulo descreve as diversas maneiras de organizar os dados conforme identificadas pelo artigo de Wickham.

### Mapa conceitual {.unnumbered}

Conhecimento prévio:

   a. Chamadas de função e de métodos;
   b. Obtenção de subconjuntos de dados;
   c. Laços;
   d. `list comprehension` (abrangência de listas).

Neste capítulo:

  * Refatoração de dados:

      a. `unpivot`/`melt`/`gather`("despivotear");
      b. `pivot`/`cast`/`spread`;
      c. Obtenção de subconjuntos;
      d. Combinação:
          1. Uso da biblioteca glob
          2. Concatenação

\newpage

### Objetivos {.unnumbered}

Este capítulo abordará:

   1. Operação de `unpivoting`/`melting`/`gathering` de colunas em linhas ("despivotear");
   2. Operação de `pivoting`/`casting`/`spreading` de linhas em colunas ("pivotear");
   3. Normalização de dados separando um dataframe em várias tabelas;
   4. Reunião de dados de várias partes.

\newpage

## Coluna contêm valores, e não variáveis

Os dados podem ter colunas que contêm valores em vez de variáveis. Em geral, é um formato conveniente para coleta e apresentação de dados.

### Mantendo uma coluna fixa

Usaremos dados do **Pew Research Center** sobre renda e religião o Estados Unidos para mostrar como trabalhar com colunas que contenham **valores** em vez de **variáveis**.

```{python}
# Importando dados Pew
pew = pd.read_csv("Data/Cap_06/pew.csv")
print(pew.head())
```

\newpage

Quando observamos esse conjunto de dados, podemos ver que nem todas as colunas são variável. Os valores relacionados à renda estão espalhados em várias colunas. O formato exibido é uma ótima opção quando apresentamos dados em uma tabela, mas, para análise de dados, a tabela precisa ser reformatada a fim de que tenhamos variáveis para religião, renda e contador.

```{python}
# Exibe somente as primeiras colunas
print(pew.iloc[:,0:6])
```

\newpage

Essa visualização dos dados também é conhecida como dados "largos" (*wide*). Para transformá-lo no formato de dados "longos" (*long*) do *tidy data*, teremos de efetuar uma operação de `unpivot`/`melt`/`gather` (depende da linguagem de programação estatística que estivermos usando) em nosso dataframe. [O Pandas tem uma função chamada `melt` que reformatará o dataframe de maneira organizada]{.underline}.

```{mermaid}
%%| label: fig-unpivot
%%| fig-cap: "Transformando wide (largo) em long (longo) data."

graph TD
    A[Wide Data] --unpivot--> B[Long Data]
    A --melt--> B
    A --gather--> B
```

`melt` aceita alguns **parâmetros**:

* `id_vars`:
  
  É um contêiner (lista, tupla, ndarray) que representa as variáveis que permanecerão inalteradas.

* `value_vars`:

   Identifica as colunas em que a operação de `melt` (ou `unpivot`) será executada. Por padrão, ela será executada em todas as colunas não especificadas no parâmetro `id_vars`.

* `var_name`:

   É uma string para o nome da nova coluna quando um `melt` é executado em `value_vars`. Por padrão, ela será chamada de `variable`.

* `value_name`:

   É uma string para o nome da nova coluna que presenta valores para `var_name`. Por padrão, ela será chamada de `value`.

\newpage

```{python}
# Não precisamos especificar um value_vars, pois queremos pivotear
#todas as colunas, exceto a coluna 'religion'
pew_long = pd.melt(pew,id_vars='religion')

print(pew_long.head())
```

```{python}
print(pew_long.tail())
```

\newpage

Podemos alterar os defaults de modo que as colunas sujeitas à operação de `melt`/`unpivot` sejam nomeadas.

```{python}
pew_long = pd.melt(
   pew,
   id_vars='religion',
   var_name='income',
   value_name='count'
   )

print(pew_long.head())
```

```{python}
print(pew_long.tail())
```

\newpage

### Mantendo várias colunas fixas

Nem todo conjunto de dados terá uma coluna que permanecerá inalterada enquanto você executa um unpivot ("despivotear") no restante das colunas.

Como exemplo, considere o conjunto de dados Billboard:

```{python}
billboard = pd.read_csv("Data/Cap_06/billboard.csv")

# Oberserve as primeiras linhas e colunas
print(billboard.iloc[0:5,0:16])
```

Podemos ver, nesse caso, que cada semana tem a própria coluna. Novamente, não há nada de errado com esse formato de dados. Talvez seja mais fácil inserir os dados nele; além disso, quando os dados são apresentados em uma tabela é muito mais rápido entender o seu significado.

\newpage

No entento, pode haver um momento em que você precisará executar um `melt` nos dados. Por exemplo, se quisesse criar uma plotagem com faceta com as classificações semanais, a variável de facetar teria de ser uma coluna do dataframe.

```{python}
billboard_long = pd.melt(
   billboard,
   id_vars=['year','artist','track','time','date.entered'],
   var_name='week',
   value_name='rating'
)

print(billboard_long.head())
```

```{python}
print(billboard_long.tail())
```

\newpage

## Colunas contendo diversas variáveis

Às vezes, as colunas em um conjunto de dados podem representar diversas variáveis. Esse formato é comumente visto quando trabalhamos com dados de saúde, por exemplo.

Para ilustrar a situação, vamos observar o conjunto de dados Ebola.

```{python}
ebola = pd.read_csv("Data/Cap_06/country_timeseries.csv")

print(ebola.columns)
```

```{python}
# Exibe linhas selecionadas
print(ebola.iloc[:5,[0,1,2,3,10,11]])
```

\newpage

[Os nomes de colunas `Cases_Guinea` e `Deaths_Guinea` na verdade contêm duas variáveis: o status individual (casos e mortes, respectivamente), assim como o nome do país, Guinea (Guiné)]{.underline}. Os dados também estão organizados em formato largo (`wide`), e a operação de `unpivot` (`melt`) deve ser executada.

```{python}
ebola_long = pd.melt(
   ebola,
   id_vars=['Date','Day']
)

print(ebola_long.head())
```

```{python}
print(ebola_long.tail())
```

\newpage

### Separar e adicionar colunas individualmente (método simples)

Conceitualmente, a coluna de interesse pode ser separada com base no underscore no nome da coluna, "_". A primeira parte será a nova coluna de status, e a segunda será a nova coluna de país. Isso exigirá um pouco de parsing e separação de string em Python.

Em Python uma string é um objeto, semelhante ao modo como o Pandas tem objetos `Series` e `DataFrame`. As `Series` podem conter métodos como `mean`, e os `DataFrames` podem conter métodos como `to_csv`.

[As strings também têm métodos]{.underline}. Nesse caso, usaremos o método `split`, [que aceita uma string e a separa com base em um dado delimitador]{.underline}. Por padrão, `split` separará a string com base em um espaço, mas podemos passar o underscore, _, em nosso exemplo.

Para ter acesso aos métodos de string, temos de usar o método de acesso `str`. Isso nos dará acesso aos métodos de string de Python e nos permitirá trabalhar em toda a coluna.

```{python}
# Obtém a coluna variável,
# acessa os métodos da string,
# e separa a coluna com base em um delimitador

variable_split = ebola_long.variable.str.split('_')

print(variable_split[:5])
```

```{python}
print(variable_split[-5:])
```

\newpage

Depois da separação com base no underscore, os valores são devolvidos em uma lista. Sabemos se tratar de uma lista (`list`) porque é assim que o método `split` funciona, mas a pista visual encontra-se no fato de o resultado estar cercado por colchetes.

```{python}
# O contêiner inteiro
print(type(variable_split))
```

```{python}
# O primeiro elemento do contêiner
print(type(variable_split[0]))
```

Agora que a coluna foi separada em várias partes, o próximo passo é atribuir essas partes a uma nova coluna. Inicialmente, porém, precisamos extrair todos os elementos com índice 0 da coluna de status e os elementos de índice 1 da coluna de país. Para isso, tamos de acessar os métodos de string novamente e então usar o método `get` para obter o índice que queremos em cada linha.

* O método `get()`:

   * **O que ele faz**: 
  
      O `str.get(i)` acessa o elemento na posição i de cada lista dentro da Series.

   * **A lógica**: 
      
      É como se você estivesse percorrendo a coluna e dizendo: "Pegue apenas o primeiro item de cada lista e coloque em uma nova coluna".

```{python}
status_value = variable_split.str.get(0)
country_value = variable_split.str.get(1)

print(status_value[:5])
```

```{python}
print(status_value[-5:])
```

```{python}
print(country_value[:5])
```

```{python}
print(country_value[-5:])
```

Agora que temos os vetores desejados, podemos adicioná-los em nosso dataframe.

```{python}
ebola_long['status'] = status_value
ebola_long['country'] = country_value

print(ebola_long.head())
```

\newpage

### Separar e combinar em um único passo (método simples)

Nesta seção, exploraremos o fato de que o vetor devolvido está na mesma ordem que nossos dados. Podemos concatenar o novo vetor aos nossos dados originais.

* O parâmetro `expand=True`

   Este parâmetro altera o tipo de objeto que o `split` devolve.

   * `expand=False` (Padrão): 
   
      Retorna uma Series de listas.

      `Ex: 0    [Cases, Guinea]`

   * `expand=True`:
  
      Retorna um DataFrame completo, onde cada parte separada vira uma coluna própria.
  
      ```
      Ex:
      | | 0 | 1 |
      |---|---|---|
      | 0 | Cases | Guinea |
      ```

* `axis` parâmetro do `concat`:

  * `axis=1`:

      **Adiciona colunas**. Você está "colando" as novas informações (status e country) à direita da sua tabela original.

   * `axis=0` (**padrão**):
  
      **Adicionaria linhas**, uma embaixo da outra.


```{python}
variable_split = ebola_long.variable.str.split('_',expand=True)
variable_split.columns = ['status','country']
ebola_parsed = pd.concat([ebola_long,variable_split], axis=1)

print(ebola_parsed.head())
```

```{python}
print(ebola_parsed.tail())
```

Explicando o código:

  * Pega a coluna variable (que tem textos como `Cases_Guinea`), corta onde tem o sublinhado (_) e cria duas colunas separadas graças ao `expand=True`.

  * Define que essas duas novas colunas agora se chamam oficialmente `status` e `country`.

  * Usa o `pd.concat` com `axis=1` para pegar essas duas novas colunas e grudá-las ao lado da sua tabela original (`ebola_long`).

\newpage

### Separar e combinar em um único passo (método mais complicado)

\newpage

## Variáveis tanto em linhas quanto em colunas
## Várias unidades de observação em uma tabela (normalização)
## Unidade de observação em várias tabelas
## Conclusão *Tidy data*

\newpage

# Referências